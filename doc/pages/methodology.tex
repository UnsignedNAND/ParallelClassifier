\chapter{Methodology}
\label{methodology}

\section{Introduction}
In the first section of this chapter the significance of the project is briefly explained. Then, according to it, the work plan is presented in the second section.  In next sections designed algorithms are described taking into consideration the possibility to provide a framework for text document categorization in parallel environment. 

\section{Significance}
Data mining algorithms play an essential role not only in business and science applications but also in medicine as second-opinion diagnostic tools. On the other hand, nowadays text documents become more and more accessible, because of digitalization of information. Taking it into consideration there is a strong need to find a tool which allows to speed up information lookup by classifying given document with certain confidence of accuracy in relevance to the set of other documents. This solution based on using data mining algorithms in parallel environment can doubtless have a wide range of applications. The motivation for this work is to provide algorithms for documents features extraction, documents clasterization and classification. Next step is to scale this solution into application based on parallel processing. This combination may lead to previously undiscovered system that allows to accelerate   document classification significantly. Many new aspects analyzed and defined in this paper may turned out to be important in the future researches and significant for development in many areas, for example:
\begin{enumerate}

\item spam filtering

\item email routing

\item language identification


\end{enumerate}


\section{Work plan}

According to results that should be achieved, the plan of the project is divided into following parts:

\begin{itemize}
\item \textbf{Task 1} Implementation and validation of algorithms for feature extraction

\item  \textbf{Task 2} Development and validation of algorithms for documents clusterization and specyfing documents groups based on testing data set

\item  \textbf{Task 3} Development and validation of algorithms for
 new document classification

\item  \textbf{Task 4} Analyzing collected data and validating accuracy of used methods

\item \textbf{Task 5} Development and validation of algorithms for parallel processing 

\item  \textbf{Task 6} Analyzing collected data and validating accuracy of parallel processing algorithms
\end{itemize}

\section{Methodology}
\subsection{Feature extraction}
For human beings, documents are an essential mean of both preserving and exchanging knowledge. Those documents store different kinds of information using written form of language. Document consists of sentences, words, notes linking one article with another, marking relation between them. While it gives us, humans, the most common way of storing information, it also rises numerous problems for machines to process and understand informations stored in such way. To overcome this issue, a number of solutions were introduced.

This section summarizes methods that has been used or considered to be used for the best feature extraction that could possibly lead to more accurate clusterization and classification.

\subsubsection{Tokenization}
First step in feature extraction process is to tokenize document. Tokenization means that ordered sentences are turned into a collection of unordered words. During this step it is possible to reduce the overall weight of corpus by deleting stop words, all special signs and converting all characters to lower case - they do not carry any information that would be considered important during clusterization and classification.

During tokenization there is possibility of losing too much information, though. Some set of words are not supposed to be divided into seperate, unordered pieces. Consider following sentance as an example:

\textquotedblleft A popular tourist destination, San Francisco is known for its cool summers\textquotedblright

Tokenizing above sentence results in:

['a', 'popular', 'tourist', 'destination,', 'san', 'francisco', 'is', 'known', 'for', 'its', 'cool', 'summers'].

As can be seen, 'San Francisco' has beed divided into two parts - 'san' and 'francisco'. In this particular case tokenization caused additional loss of information, as 'san' and 'francisco', when presented seperatly, might have another meaing. To bypass this issue, n-grams can be used.

n-grams posses some advatnages over tokens. Tokens hold information about single word, while n-grams are a sequence of N>=1 consecutive words that appear in text. Example presented above turned into n-grams with N=3 will result in:

[['A', 'popular', 'tourist'], ['popular', 'tourist', 'destination,'], ['tourist', 'destination,', 'San'], ['destination,', 'San', 'Francisco'], ['San', 'Francisco', 'is'], ['Francisco', 'is', 'known'], ['is', 'known', 'for'], ['known', 'for', 'its'], ['for', 'its', 'cool'], ['its', 'cool', 'summers']]

\subsubsection{Stemming and lemmatization}
Important step in feature extraction process is word stemming and lemmatization. They both allow for derived words that might be in inflectional forms and sometimes derivationally related forms of a word to be brought to a common root. 

Stemming is an approach based on different algorithms for calculating root of a given word. It consist of methods like lookup tables, suffix and postfix stripping algorithms. While this method is efficent, it's accuracy suffers. Root might not always be a lexically correct form, but it still enables both counting occurences of all forms of given word across the document and helps to reduce the overall weight of corpus.

Lemmatization is based on different dictionaries to find root of a word. The most common dictionary used in lemmatization process is WordNet.

\subsubsection{Term Frequency and Inverse Document Index}
In order to define document's topic it is essential to find relevant keywords. To achieve this, TF-IDF term weighting algorithm can be utilized. It is a numerical statistic method which reflects how important is a given word to a document in corpus. Value calculated by this method increases proportionally to the number of times a given word appears in a document, but is reduced by the frequency of the word in corpus. This approach allows words that are important for one document and appear frequently in it but rarely in corpus to score higher than ones that appear frequently in whole corpus.

Consider following sentences as an examples for further presentation of TD-IDF:

\begin{table}[ht]
	\centering
	\caption{Sample documents for TF-IDF presentation}
	\label{my-label}
	\begin{tabular}{ll}
		Document 1. & I wasted time, and now doth time waste me. \\
		Document 2. & There's a time for all things.             \\
		Document 3. & Time heals all wounds.                     \\
		Document 4. & He in peace is wounded, not in war.       
	\end{tabular}
\end{table}

\begin{itemize}
	\item \textbf{TF - Term Frequency} - is a measure of frequency with which given word occurs in document. Words that occur more often in a document have higher value than those that do not. Returned value is normalized.
	\[TF\textsubscript{word}=\frac{word\ occurence\ count}{total\ words\ in\ document}\]
	\item \textbf{IDF - Inverse Document Frequency} - it can be observed that certain words that do not carry much information appear very frequently across all documents written in English. Example of these words are: 'a', 'an', 'the' and many others. But because not only articles and stopwords can turn out to be redundant, IDF was introduced to counter the emphasis that these words carry with them. IIDF diminishes the weight of words that occur frequently and increases the weight of words that occur rarely across all documents in corpus.
	\[IDF\textsubscript{word}=1+log_e \frac{total\ number\ of\ documents\ in\ corpus}{number\ of\ documents\ with\ word\ in\ it}\]
	\item \textbf{TF-IDF - Term Frequency-Inverse Document Frequency}
	\[TFIDF\textsubscript{word}=TF\textsubscript{word} * IDF\textsubscript{word}\]
\end{itemize}

\subsection{Clusterization}
\subsection{Classification}