\chapter{Methodology}
\label{methodology}

\section{Introduction}
In the first section of this chapter the significance of a project is briefly explained. Then, according to it, the work plan is presented in the second section.  In next sections designed algorithms are described taking into consideration the possibility to provide a framework for text document categorization in parallel environment. 

\section{Significance}
Data mining algorithms play an essential role not only in business and science applications but also in medicine as second-opinion diagnostic tools. On the other hand, nowadays text documents become more and more accessible, because of information digitalization. Taking it into consideration there is a strong need to find a tool which allows to speed up information lookup by classifying given document with certain confidence of accuracy in relevance to the set of other documents. This solution based on using data mining algorithms in parallel environment can doubtless have a wide range of applications. The motivation for this work is to provide algorithms for documents features extraction, documents clasterization and classification. Next step is to scale this solution into application based on parallel processing. This combination may lead to previously undiscovered system that allows to accelerate document classification significantly. Many new aspects analyzed and defined in this paper may turned out to be important in the future researches and significant for development in many areas, for example:
\begin{enumerate}

\item spam filtering

\item email routing

\item language identification


\end{enumerate}


\section{Work plan}

According to results that should be achieved, the plan of the project is divided into following parts:

\begin{itemize}
\item \textbf{Task 1} Implementation and validation of algorithms for feature extraction

\item  \textbf{Task 2} Development and validation of algorithms for documents clusterization and specyfing documents groups based on testing data set

\item  \textbf{Task 3} Development and validation of algorithms for
 new document classification

\item  \textbf{Task 4} Analyzing collected data and validating accuracy of used methods

\item \textbf{Task 5} Development and validation of algorithms for parallel processing 

\item  \textbf{Task 6} Analyzing collected data and validating accuracy of parallel processing algorithms
\end{itemize}

\section{Methodology}
\subsection{Feature extraction}
For human beings, documents are an essential mean of both preserving and exchanging knowledge. Those documents store different kinds of information using written form of language. Document consists of sentences, words, notes linking one article with another, marking relation between them. While it gives us, humans, the most common way of storing information, it also rises numerous problems for machines to process and understand informations stored in such way. To overcome this issue, a number of solutions were introduced.

This section summarizes methods that has been used or considered to be used for the best feature extraction that could possibly lead to more accurate clusterization and classification.

\subsubsection{Tokenization}
First step in feature extraction process is to tokenize document. Tokenization means that ordered sentences are turned into a collection of unordered words. During this step it is possible to reduce the overall weight of corpus by deleting stop words (def. \ref{stop_words}), all special signs and converting all characters to lower case - they do not carry any information that would be considered important during clusterization and classification.

 \begin{definition}[Stop words]
 	\label{stop_words}
 	In documents analysis, stop words are words which are filtered out before or after processing of natural language text. They usually refer to the most common words in a specific language, so creating universal list of stop words is quite challenging.
 \end{definition}


During tokenization there is possibility of losing too much information, though. Some set of words are not supposed to be divided into seperate, unordered pieces. Consider following sentance as an example:\\

\textquotedblleft \textit{A popular tourist destination, San Francisco is known for its cool summers.}\textquotedblright\\

Tokenizing above sentence results in:\\

\textit{['a', 'popular', 'tourist', 'destination,', 'san', 'francisco', 'is', 'known', 'for', 'its', 'cool', 'summers'].}\\

As can be seen, \textit{'San Francisco'} has beed divided into two parts - \textit{'san'} and \textit{'francisco'}. In this particular case tokenization caused additional loss of information, as 'san' and 'francisco', when presented seperatly, might have another meaing. To bypass this issue, n-grams can be used.\\

\textbf{n-grams}

n-grams posses some advatnages over tokens. Tokens hold information about single word, while n-grams are a sequence of N $>$=1 consecutive words that appear in text. An n-gram of size 1 is referred to as a unigram, size 2 is a bigram (or a digram), size 3 is a trigram. Example presented above turned into n-grams with N=3 will result in:\\

\textit{[['A', 'popular', 'tourist'], ['popular', 'tourist', 'destination,'], ['tourist', 'destination,', 'San'], ['destination,', 'San', 'Francisco'], ['San', 'Francisco', 'is'], ['Francisco', 'is', 'known'], ['is', 'known', 'for'], ['known', 'for', 'its'], ['for', 'its', 'cool'], ['its', 'cool', 'summers']]}\\

As can be seen, resulted trigram is a contiguous sequence of 3 items. An n-gram model is a type of probabilistic language model for predicting the next item in text sequence. These models have recently gained a lot of popularity for using them in probability, communication theory, computational linguistics and data compression.

\subsubsection{Stemming and lemmatization}
Important step in feature extraction process is word stemming and lemmatization. They both allow for derived words that might be in inflectional forms (def. \ref{inflectional}) and sometimes derivationally related forms of a word to be brought to a common root (def. \ref{root}). 

 \begin{definition}[Inflectional form]
 	\label{inflectional}
In grammar, inflection or inflexion is the modification of a word using a prefix, suffix or infix, or another internal modification such as a vowel change. The aim is to express various grammatical categories for example tense, case, voice, aspect, person, number, gender, and mood. 
\end{definition}

 \begin{definition}[Root word]
 	\label{root}
A root word is a word that does not have a prefix (before the word) or a suffix (after a word). The root word is the primary lexical unit of a word. It defines a word family. A root is often called base word, which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. 
Some common root words, their meanings and words that are formed from this blocks are presented below:
\begin{itemize}
	\item Act  - move or do - action, activity, transaction
	\item Ambul -  move or walk - amble, ambulant
	\item Auto -  self or same -  automate, automatic
	\item Cardio -  heart - cardiology
	\item Cede -  go -  exceed, accessible
	\item Counter -  against or opposite - counteract, counterpoint 
	\item Demo -  people - democracy, demographic
	\item Derma -  skin - dermatology, epidermis
	\item Equi -  equal - equity
	\item Semi -  half - semicircle 
\end{itemize}
\end{definition}

Stemming is an approach based on different algorithms for calculating root of a given word. It consist of methods like lookup tables, suffix and postfix stripping algorithms. While this method is efficent, its accuracy suffers. Root might not always be a lexically correct form, but it still enables both counting occurences of all forms of given word across the document and helps to reduce the overall weight of corpus.

Lemmatization is based on different dictionaries to find root of a word. The most common dictionary used in lemmatization process is WordNet.

\subsubsection{Term Frequency and Inverse Document Index}
In order to define document's topic it is essential to find relevant keywords. To achieve this, TF-IDF term weighting algorithm can be utilized. It is a numerical statistic method which reflects how important is a given word to a document in corpus. Value calculated by this method increases proportionally to the number of times a given word appears in a document, but is reduced by the frequency of the word in corpus. This approach allows words that are important for one document and appear frequently in it but rarely in corpus to score higher than ones that appear frequently in whole corpus.

Consider following sentences and matrix of tokens as an examples for further presentation of TD-IDF:

\begin{table}[ht]
	\centering
	\caption{TF-IDF - Example documents}
	\label{tf_idf_example_sentences}
	\begin{tabular}{@{}ll@{}}
		Document 1. & A species is a kind of organism. \\
		Document 2. & An organism is a living thing.   \\
		Document 3. & Bacteria is a living organism.   \\
		Document 4. & A is the letter.                
	\end{tabular}
\end{table}

\begin{itemize}
	\item \textbf{TF - Term Frequency} - is a measure of frequency with which given word occurs in document. Because every document is different in length and thus longer documents will contain words with higher occurence count, value returned by TF is usually normalized. Words that occur more often in a document have higher value than those that do not.
	
	As can be observed in \ref{tf_idf_tf_freq}, matrix created in a process tends to contain many empty cells (word does not occur in a document). It might be worth to consider using sparse matrix implementation in order to preserve memory.
	\[TF\textsubscript{word}=\frac{word\ occurence\ count}{total\ words\ in\ document}\]	
\begin{table}[H]
	\centering
	\caption{TF - Words frequencies}
	\label{tf_idf_tf_freq}
	\begin{tabular}{@{}lccccccccccc|c|@{}}
		\toprule
		& \multicolumn{1}{l}{a} & \multicolumn{1}{l}{species} & \multicolumn{1}{l}{is} & \multicolumn{1}{l}{kind} & \multicolumn{1}{l}{of} & \multicolumn{1}{l}{organism} & \multicolumn{1}{l}{living} & \multicolumn{1}{l}{thing} & \multicolumn{1}{l}{bacteria} & \multicolumn{1}{l}{the} & \multicolumn{1}{l|}{letter} & \multicolumn{1}{l|}{Total} \\ \midrule
		D1 & 2                     & 1                           & 1                      & 1                        & 1                      & 1                            &                            &                           &                              &                         &                             & 7                          \\ \cmidrule(l){13-13} 
		D2 & 2                     &                             & 1                      &                          &                        & 1                            & 1                          & 1                         &                              &                         &                             & 6                          \\ \cmidrule(l){13-13} 
		D3 & 1                     &                             & 1                      &                          &                        & 1                            & 1                          &                           & 1                            &                         &                             & 5                          \\ \cmidrule(l){13-13} 
		D4 & 1                     &                             & 1                      &                          &                        &                              &                            &                           &                              & 1                       & 1                           & 4                          \\ \bottomrule
	\end{tabular}
\end{table}
\begin{table}[H]
	\centering
	\caption{TF - Normalized words frequencies}
	\label{tf_idf_tf_norm_freq}
	\begin{tabular}{@{}lccccccccccc@{}}
		\toprule
		& \multicolumn{1}{l}{a} & \multicolumn{1}{l}{species} & \multicolumn{1}{l}{is} & \multicolumn{1}{l}{kind} & \multicolumn{1}{l}{of} & \multicolumn{1}{l}{organism} & \multicolumn{1}{l}{living} & \multicolumn{1}{l}{thing} & \multicolumn{1}{l}{bacteria} & \multicolumn{1}{l}{the} & \multicolumn{1}{l}{letter} \\ \midrule
		D1 & 0.29                  & 0.14                        & 0.14                   & 0.14                     & 0.14                   & 0.14                         &                            &                           &                              &                         &                            \\
		D2 & 0.33                  &                             & 0.16                   &                          &                        & 0.16                         & 0.16                       & 0.16                      &                              &                         &                            \\
		D3 & 0.20                  &                             & 0.20                   &                          &                        & 0.20                         & 0.20                       &                           & 0.20                         &                         &                            \\
		D4 & 0.25                  &                             & 0.25                   &                          &                        &                              &                            &                           &                              & 0.25                    & 0.25                       \\ \bottomrule
	\end{tabular}
\end{table}
	
	\item \textbf{IDF - Inverse Document Frequency} - it can be observed that certain words that do not carry much information appear very frequently across all documents written in English. Example of these words are: 'a', 'an', 'the' and many others. But because not only articles and stopwords can turn out to be redundant, IDF was introduced to counter the emphasis that these words carry with them. IIDF diminishes the weight of words that occur frequently and increases the weight of words that occur rarely across all documents in corpus.

	
	\begin{table}[H]
		\centering
		\caption{My caption}
		\label{my-label}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Term     & IDF  \\ \midrule
			is       & 1.00 \\
			species  & 2.37 \\
			a        & 1.00 \\
			organism & 1.29 \\
			the      & 2.39 \\
			thing    & 2.39 \\
			letter   & 2.39 \\
			of       & 2.39 \\
			kind     & 2.39 \\
			bacteria & 2.39 \\
			living   & 1.69 \\ \bottomrule
		\end{tabular}
	\end{table}
	
		\[IDF\textsubscript{word}=1+log_e \frac{total\ number\ of\ documents\ in\ corpus}{number\ of\ documents\ with\ word\ in\ it}\]

	\item \textbf{TF-IDF - Term Frequency-Inverse Document Frequency}
	Final result for TF-IDF is achieved by multiplying TF and IDF values.
	\[TFIDF\textsubscript{word}=TF\textsubscript{word} * IDF\textsubscript{word}\]
\end{itemize}

\subsubsection{Cosine similarity}

\subsection{Clusterization}
\subsection{Classification}
\section{Conclusion}
This chapter includes a complete description of the methodology used to prepare analysis of the requirements for created system. Parallelization of documents classification is the main scientific part of proposed solution. Providing algorithms for documents features extraction, clasterization and classification in the application based on parallel processing may lead to previously undiscovered system that allows to accelerate document classification significantly. 