\chapter{Methodology}
\label{methodology}

\section{Introduction}
In the first section of this chapter the significance of a project is briefly explained. Then, according to it, the work plan is presented in the second section.  In next sections designed algorithms are described taking into consideration the possibility to provide a framework for text document categorization in parallel environment. 

\section{Significance}
Data mining algorithms play an essential role not only in business and science applications but also in medicine as second-opinion diagnostic tools. On the other hand, nowadays text documents become more and more accessible, because of information digitalization. Taking it into consideration there is a strong need to find a tool which allows to speed up information lookup by classifying given document with certain confidence of accuracy in relevance to the set of other documents. This solution based on using data mining algorithms in parallel environment can doubtless have a wide range of applications. The motivation for this work is to provide algorithms for documents features extraction, documents clasterization and classification. Next step is to scale this solution into application based on parallel processing. This combination may lead to previously undiscovered system that allows to accelerate document classification significantly. Many new aspects analyzed and defined in this paper may turned out to be important in the future researches and significant for development in many areas, for example:
\begin{enumerate}

\item spam filtering

\item email routing

\item language identification


\end{enumerate}


\section{Work plan}

According to results that should be achieved, the plan of the project is divided into following parts:

\begin{itemize}
\item \textbf{Task 1} Implementation and validation of algorithms for feature extraction

\item  \textbf{Task 2} Development and validation of algorithms for documents clusterization and specyfing documents groups based on testing data set

\item  \textbf{Task 3} Development and validation of algorithms for
 new document classification

\item  \textbf{Task 4} Analyzing collected data and validating accuracy of used methods

\item \textbf{Task 5} Development and validation of algorithms for parallel processing 

\item  \textbf{Task 6} Analyzing collected data and validating accuracy of parallel processing algorithms
\end{itemize}

\section{Methodology}
\subsection{Feature extraction}
For human beings, documents are an essential mean of both preserving and exchanging knowledge. Those documents store different kinds of information using written form of language. Document consists of sentences, words, notes linking one article with another, marking relation between them. While it gives us, humans, the most common way of storing information, it also rises numerous problems for machines to process and understand informations stored in such way. To overcome this issue, a number of solutions were introduced.

This section summarizes methods that has been used or considered to be used for the best feature extraction that could possibly lead to more accurate clusterization and classification.

\subsubsection{Tokenization}
First step in feature extraction process is to tokenize document. Tokenization means that ordered sentences are turned into a collection of unordered words. During this step it is possible to reduce the overall weight of corpus by deleting stop words (def. \ref{stop_words}), all special signs and converting all characters to lower case - they do not carry any information that would be considered important during clusterization and classification.

 \begin{definition}[Stop words]
 	\label{stop_words}
 	In documents analysis, stop words are words which are filtered out before or after processing of natural language text. They usually refer to the most common words in a specific language, so creating universal list of stop words is quite challenging.
 \end{definition}


During tokenization there is possibility of losing too much information, though. Some set of words are not supposed to be divided into seperate, unordered pieces. Consider following sentance as an example:\\

\textquotedblleft \textit{A popular tourist destination, San Francisco is known for its cool summers.}\textquotedblright\\

Tokenizing above sentence results in:\\

\textit{['a', 'popular', 'tourist', 'destination,', 'san', 'francisco', 'is', 'known', 'for', 'its', 'cool', 'summers'].}\\

As can be seen, \textit{'San Francisco'} has beed divided into two parts - \textit{'san'} and \textit{'francisco'}. In this particular case tokenization caused additional loss of information, as 'san' and 'francisco', when presented seperatly, might have another meaing. To bypass this issue, n-grams can be used.\\

\textbf{n-grams}

n-grams posses some advatnages over tokens. Tokens hold information about single word, while n-grams are a sequence of N $>$=1 consecutive words that appear in text. An n-gram of size 1 is referred to as a unigram, size 2 is a bigram (or a digram), size 3 is a trigram. Example presented above turned into n-grams with N=3 will result in:\\

\textit{[['A', 'popular', 'tourist'], ['popular', 'tourist', 'destination,'], ['tourist', 'destination,', 'San'], ['destination,', 'San', 'Francisco'], ['San', 'Francisco', 'is'], ['Francisco', 'is', 'known'], ['is', 'known', 'for'], ['known', 'for', 'its'], ['for', 'its', 'cool'], ['its', 'cool', 'summers']]}\\

As can be seen, resulted trigram is a contiguous sequence of 3 items. An n-gram model is a type of probabilistic language model for predicting the next item in text sequence. These models have recently gained a lot of popularity for using them in probability, communication theory, computational linguistics and data compression.

\subsubsection{Stemming and lemmatization}
Important step in feature extraction process is word stemming and lemmatization. They both allow for derived words that might be in inflectional forms (def. \ref{inflectional}) and sometimes derivationally related forms of a word to be brought to a common root (def. \ref{root}). 

 \begin{definition}[Inflectional form]
 	\label{inflectional}
In grammar, inflection or inflexion is the modification of a word using a prefix, suffix or infix, or another internal modification such as a vowel change. The aim is to express various grammatical categories for example tense, case, voice, aspect, person, number, gender, and mood. 
\end{definition}

 \begin{definition}[Root word]
 	\label{root}
A root word is a word that does not have a prefix (before the word) or a suffix (after a word). The root word is the primary lexical unit of a word. It defines a word family. A root is often called base word, which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. 
Some common root words, their meanings and words that are formed from this blocks are presented below:
\begin{itemize}
	\item Act  - move or do - action, activity, transaction
	\item Ambul -  move or walk - amble, ambulant
	\item Auto -  self or same -  automate, automatic
	\item Cardio -  heart - cardiology
	\item Cede -  go -  exceed, accessible
	\item Counter -  against or opposite - counteract, counterpoint 
	\item Demo -  people - democracy, demographic
	\item Derma -  skin - dermatology, epidermis
	\item Equi -  equal - equity
	\item Semi -  half - semicircle 
\end{itemize}
\end{definition}

Stemming is an approach based on different algorithms for calculating root of a given word. It consist of methods like lookup tables (def. \ref{lookup_table}), suffix (def. \ref{suffix}) and postfix (analogically) stripping algorithms. While this method is efficient, its accuracy suffers. Root might not always be a lexically correct form, but it still enables both counting occurrences of all forms of given word across the document and helps to reduce the overall weight of corpus.

 \begin{definition}[Lookup table]
 	\label{lookup_table}
In computer science, a lookup table is an array that allows to reduce runtime computation by using a simpler array indexing operation. Shortening of processing time can be significant, as retrieving a value from memory is often faster than undergoing an computation or input/output operation. The table can be precalculated and stored in static program storage, calculated during a program's initialization phase (memoization), or even stored in hardware.
The advantages of the lookup table is that it is simple, fast, and can handle exceptions. The disadvantages include large size of a table and possibility of loosing some words. All inflected forms must be explicitly listed in the table and sometimes new or unfamiliar words are not handled, even if they are perfectly regular. 
\end{definition}

 \begin{definition}[Suffix stripping]
 	\label{suffix}
Suffix stripping algorithms rely on a smaller list of "rules" than a lookup table (a lookup table consists of inflected forms and root form relations). This shorter list used in suffix stripping provides a path for the algorithm to find its root form from an input form. Some examples of the rules are presented below:
\begin{itemize}
	\item if the word ends in 'ed', remove the 'ed'
	\item if the word ends in 'ing', remove the 'ing'
	\item if the word ends in 'ly', remove the 'ly'
\end{itemize}
 \end{definition}

 On the other hand, stripping algorithms have been shown to have a poor performance when dealing with exceptional relations (for example 'ran' and 'run'). The results given by suffix stripping algorithms are limited to those which have well defined suffixes. This is a problem, as not all words have a well formulated set of rules. Lemmatisation (def. \ref{lemmatization}) attempts to improve upon this challenge.
It is based on different dictionaries to find a root of a word. The difference between lemmatization and stemming is that a stemmer operates on a single word without any context and without knowledge about part of a speech. On the other hand, stemmers are easier to implement and this algorithm is usually faster.

For example:
\begin{itemize}
	\item The word worse has lemma bad. This relation is not found by stemming and it requires a look-up table.
	\item The word walk is the root form of walking, and it can be matched in both stemming and lemmatisation.
	\item Depending on a context, the word meeting can be either the root of a noun or a form of a verb. For example 'in our last meeting' and 'they are meeting tomorrow'. In this case lemmatisation can select the appropriate lemma depending on the context, while stemming can't.
\end{itemize}

The most common dictionary used in lemmatization process is WordNet.

 \begin{definition}[Lemmatization]
 	\label{lemmatization}
In linguistics, a lemmatization is the process of grouping together the various inflected word forms so they can be analysed as a single item. In other words, it is a process of determining the lemma (def. \ref{lemma}) for a given word.
 \end{definition}
 
  \begin{definition}[Lemma]
  	\label{lemma}
  A lemma is the canonical form, dictionary form, or citation form of a word. For example the verb 'to walk' may be presented as walk, walked, walks, walking. The base form walk, which might be looked up in a dictionary, is called the lemma. The combination of the base form with the part of speech is called the lexeme. Another example include run, runs, ran and running are forms of the same lexeme, with run as the lemma. 
  \end{definition}
 
\subsubsection{Term Frequency and Inverse Document Index}
In order to define document's topic it is essential to find relevant keywords. To achieve this, TF-IDF term weighting algorithm can be utilized. It is a numerical statistic method which reflects how important is a given word to a document in corpus. Value calculated by this method increases proportionally to the number of times a given word appears in a document, but is reduced by the frequency of the word in corpus. This approach allows words that are important for one document and appear frequently in it but rarely in corpus to score higher than ones that appear frequently in whole corpus.

Consider following sentences and matrix of tokens as an examples for further presentation of TD-IDF (tokens were lemmatized and stopwords were removed, for the sake of simplicity of visualization n-grams were not used):

\begin{table}[H]
	\centering
	\caption{TF-IDF - Example documents}
	\label{tf_idf_example_sentences}
	\begin{tabular}{@{}lll@{}}
		\toprule
		& Content                                     & Terms      \\ \midrule
		Document 1. & The sky is blue.                            & sky blue               \\
		Document 2. & The sun is bright.                          & sun bright             \\
		Document 3. & The sun in the sky is bright.               & sun sky bright         \\
		Document 4. & We can see the shining sun, the bright sun. & sun see bright shining \\ \bottomrule
	\end{tabular}
\end{table}

\begin{itemize}
	\item \textbf{TF - Term Frequency} - is a measure of frequency with which given word occurs in document. Because every document is different in length and thus longer documents will contain words with higher occurrence count, value returned by TF is usually normalized. Words that occur more often in a document have higher value than those that do not.
	
	As can be observed in \ref{tf_idf_tf_freq}, matrix created in a process tends to contain many empty cells (word does not occur in a document). It might be worth to consider using sparse matrix implementation in order to preserve memory.
	\[TF\textsubscript{word}=\frac{word\ occurence\ count}{total\ words\ in\ document}\]	
\begin{table}[H]	
	\centering
	\caption{TF - Words frequencies}
	\label{tf_idf_tf_freq}
\begin{tabular}{@{}lllllll@{}}
	\toprule
	& blue & sun & sky & see & bright & shining \\ \midrule
	Document 1. & 1    &     & 1   &     &        &         \\
	Document 2. &      & 1   &     &     & 1      &         \\
	Document 3. &      & 1   & 1   &     & 1      &         \\
	Document 4. &      & 2   &     & 1   & 1      & 1       \\ \bottomrule
\end{tabular}
\end{table}
\begin{table}[H]
	\centering
	\caption{TF - Normalized words frequencies}
	\label{tf_idf_tf_norm_freq}
\begin{tabular}{@{}lllllll@{}}
	\toprule
	& blue & sun & sky & see & bright & shining \\ \midrule
	Document 1. & 0.5    &     & 0.5   &     &        &         \\
	Document 2. &      & 0.5   &     &     & 0.5      &         \\
	Document 3. &      & 0.333   & 0.333   &     & 0.333      &         \\
	Document 4. &      & 0.4   &     & 0.2   & 0.2      & 0.2       \\ \bottomrule
\end{tabular}
\end{table}
	
	\item \textbf{IDF - Inverse Document Frequency} - it can be observed that certain words that do not carry much information appear very frequently across all documents written in English. Example of these words are: 'a', 'an', 'the' and many others. But because not only articles and stop words can turn out to be redundant, IDF was introduced to counter the emphasis that these words carry with them. IDF diminishes the weight of words that occur frequently and increases the weight of words that occur rarely across all documents in corpus.

	
	\begin{table}[H]
		\centering
		\caption{Terms with IDF values assigned}
		\label{tf_idf_idf}
	\begin{tabular}{@{}ll@{}}
		\toprule
		Term     & IDF   \\ \midrule
		blue     & 2.386 \\
		sun      & 1.288 \\
		sky      & 1.693 \\
		see      & 2.386 \\
		bright   & 1.288 \\
		shinning & 2.386 \\ \bottomrule
	\end{tabular}
	\end{table}
	
		\[IDF\textsubscript{word}=1+log_e \frac{total\ number\ of\ documents\ in\ corpus}{number\ of\ documents\ with\ word\ in\ it}\]

	\item \textbf{TF-IDF - Term Frequency-Inverse Document Frequency}
	Final result for TF-IDF is achieved by multiplying TF and IDF values.
	\[TFIDF\textsubscript{word}=TF\textsubscript{word} * IDF\textsubscript{word}\]
	
	For the documents presented above, TF-IDF values will be:

	\begin{table}[H]
		\centering
		\caption{Final result of TF-IDF}
		\label{tf_idf}
		\begin{tabular}{lllllll}
			\hline
			& blue  & sun   & sky   & see   & bright & shining \\ \hline
			Document 1. & 1.193 &       & 0.847 &       &        &         \\
			Document 2. &       & 0.644 &       &       & 0.644  &         \\
			Document 3. &       & 0.429 & 0.429 &       & 0.564  &         \\
			Document 4. &       & 0.515 &       & 0.477 & 0.258  & 0.477   \\ \hline
		\end{tabular}
	\end{table}
	
\end{itemize}

\subsubsection{Cosine similarity}
 The documents clustering can result in faster data analysis, such  as  information retrieval  and  information  extraction,  by  grouping  similar kind  of  information. While performing clusterization, a similarity between a pair of objects has to be defined. Accurate clustering requires a definition of it in terms of either similarity or distance. Different similarity and distance measures have already been described and used, for example Euclidean  distance or cosine  similarity. Cosine  similarity  is  one  of  the  most  popular  similarity  measure and it is often  applied for  text  documents clusterization. The main advantage of the cosine similarity is its independence of document length. For two documents \(d_i\) and \(d_j\), the cosine similarity between them can be defined as
\[cos(d_i, d_j)=\frac{d_i . d_j}{|| d_i || ||d_j ||}\]
The above equation can be simplified, because the document vectors have unit length
\[cos(d_i, d_j)={d_i . d_j}\]
As can be expected, documents are identical if value of cosine similarity is 1, and if the 
document vectors are orthogonal to each other this value is 0 and it means that documents are totally different.
In proposed solution this metric could be used to define a level of similarity between a pair of objects in order to group together nearest documents.
\subsection{Clusterization}
Once documents are in a form of vectors it is possible to prepare data clusters. For this, one of the most frequenlty used unsupervised methods is the K-means algorithm. It has linear time complexity in relevance to the number of documents and it is possible to run it concurrently on multiple threads. 

First step in K-means is to select K documents that will function as centers of groups. Most often these centers are selected randomly, which makes each run of K-means a bit different.
Knowing the distances between each pair of documents, we divide them into K groups. Each group is a set of documents with the distances closest to center of each group. Once groups are created, new centers are selected by looking for a document that is closest to the calculated average of document distances in its cluster. We repeat these steps until centers are still - during iteration new centers are the same as previous ones. 
More detailed implementation of a single-process K-means was described in \ref{k_means_algorithm}.

There are many variants of K-means parallel implementation, mostly depending on architecture of the solution. In our implementation, the following steps are proposed:

\begin{enumerate}
	\item \textbf{Initialization} - in this step N processes are initialized.
	Each of processes receives own copy of all documents represented by a single matrix containing calculated vectors. If processes are running on seperate nodes and document set is large, it should be considered to compress data to save time while sending it over network.
	
	\item \textbf{Select K centers}
	Master process selects K centers and distributes them to processes. With an assumption that each process has an equal processing power, each process receives \(\frac{K}{N}\) centers.
	
	\item \textbf{Computing distances between documents and centers}
	Each process calculates distances between each pair of documents and centers that were received in a previous steps. This partial result is sent back to master process.
	
	\item \textbf{Assigning documents to groups}
	Master process using the distances received from other processes, can assign each document to the closest group created around one of K documents selected previously.
	
	\item \textbf{Calculating new centers for the groups}
	Once the gorups are formed, next step is to find new centers. For each group, master process calculates mean value of distances between vectors that form it. It is possible that calculated mean value does not indicate any document directly, so the one with closest distance to it is selected.
	
	\item \textbf{Sending updated centers}
	Master process sends updated group centers to other processes. Repeat step 4.
	
	\item \textbf{Ending condition}
	If groups centers did not change in past iteration - clusterization has finished.
\end{enumerate}

\subsection{Classification}

\section{Conclusion}
This chapter includes a complete description of the methodology used to prepare analysis of the requirements for created system. Parallelization of documents classification is the main scientific part of proposed solution. Providing algorithms for documents features extraction, clasterization and classification in the application based on parallel processing may lead to previously undiscovered system that allows to accelerate document classification significantly. 