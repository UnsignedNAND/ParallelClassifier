\chapter{Implementation} \label{implementation}

\section{Introduction}
The presented solution uses a master-slave architecture in order to provide a scalable platform with means of parallel task execution. This chapter describes a code implementation for all of the algorithms and methods used in text document clusterization and classification process.


\section{Implementation}
\subsection{Preparing documents}
The first major step of the designed application is to prepare documents for further processing. This task includes loading documents from the source, converting them into feature matrix, reducing dimmensionality and finally measuring similarity between two documents.

\subsubsection{Reading and parsing documents}

Whole task is orchestreted by the \textit{parse()} method of the main process. It creates all processes involved in reading and parsing documents:
\begin{itemize}
\item a Reader process, 
\item multiple Parser processes, 
\item a single IDF process.
\end{itemize}  It also initializes communication channels for child processes that are used to exchange information - queue, pipe and event objects are utilized.

\begin{lstlisting}[language=Python, caption={Main.parse() - Main process method for conducting document parsing}, label={lst:main:parse}]
def parse(self):
    global parsed_docs
    global largest_id
    global tokens_idf

    # initialize communication

    queue_unparsed_docs = multiprocessing.Queue()
    queue_parsed_docs = multiprocessing.Queue()
    pipe_tokens_to_idf_parent, pipe_tokens_to_idf_child = multiprocessing.Pipe()
    pipes_tokens_to_processes_parent = []
    pipes_tokens_to_processes_child = []
    for i in range(PROCESSES):
        pipe_tokens_to_processes_parent, pipe_tokens_to_processes_child = \
            multiprocessing.Pipe()
        pipes_tokens_to_processes_parent.append(pipe_tokens_to_processes_parent)
        pipes_tokens_to_processes_child.append(pipe_tokens_to_processes_child)

    # additional pipe to transfer IDF values from IDF process to master
    pipe_idf_master_parent, pipe_idf_master_child = multiprocessing.Pipe()
    pipes_tokens_to_processes_parent.append(pipe_idf_master_parent)

    event = multiprocessing.Event()
    event.clear()

    # set up processes

    ps_reader = Reader(q_unparsed_docs=queue_unparsed_docs)
    ps_parsers = create_parsers(
        queue_unparsed_documents=queue_unparsed_docs,
        pipe_tokens_to_idf_child=pipe_tokens_to_idf_child,
        event=event,
        pipes_tokens_to_processes_child=pipes_tokens_to_processes_child,
        queue_parsed_docs=queue_parsed_docs,
        process_num=PROCESSES
    )
    ps_idf = IDF(
        pipe_tokens_to_idf_parent=pipe_tokens_to_idf_parent,
        docs_num=int(CONF['general']['item_limit']),
        event=event,
        pipes_tokens_to_processes_parent=pipes_tokens_to_processes_parent,
        process_num=PROCESSES
    )

    # read all the articles from XML and do TF-IDF
    ps_reader.start()

    LOG.info("Started processing documents using {0} processes".format(
        PROCESSES))
    for ps_parser in ps_parsers:
        ps_parser.start()
    ps_idf.start()

    # receive tokens IDF values from IDF process
    tokens_idf = pipe_idf_master_child.recv()

    ps_reader.join()
    ps_idf.join()

    # processes will not end until all the data is not received
    parsed_docs = self._receive_parsed_docs(queue_parsed_docs)

    for ps_parser in ps_parsers:
        ps_parser.join()
\end{lstlisting}

The first step of the presented workflow is connected with loading documents into a system. This implementation allows for it in two alternative ways:
\begin{itemize}
\item parse database dump represented in a form of a large XML file 
\item load them from the prepared database.
\end{itemize}  The number of solutions for loading documents can be easily expanded as their functions are self-contained and the only output are the documents. As long as any algorithm provides documents in a proper form as its output, it can be used in this step. To prevent workflow from blocking, this task is conducted by the Reader class in a single, seperate process. Main method of the Reader class is presented on listing \ref{lst:reader:run}.


\begin{lstlisting}[language=Python, caption={Reader.run() - Reader class process main method}, label={lst:reader:run}]
def run(self):
    if str2bool(CONF['general']['load_from_db']):
        LOG.info('Loading from DB')
        self._read_from_db()
    else:
        self._read_from_file()
\end{lstlisting}

Loading documents from a database (listing \ref{lst:reader:db}) is the most basic and straight forward solution. This method consist of making a connection to a database and reading data found in \textbf{Doc} table into the Page class instance, that would serve as a container for a single document throughout the whole  application lifecycle. The created Page objects are put into queue for further processing.

\begin{lstlisting}[language=Python, caption={Reader.\_read\_from\_db() - reading docs from database}, label={lst:reader:db}]
def _read_from_db(self):
	Db.init()
	session = Db.create_session()
	pages = session.query(Models.Doc).all()
	
	for page in pages:
		p = Page()
		p.id = page.id
		p.title = page.title
		p.content = page.text
	self._q_unparsed_docs.put(p)
	# A pill for other threads
	self._q_unparsed_docs.put(None)
\end{lstlisting}

The second approach (listing \ref{lst:reader:xml}) includes loading documents from a database dump in a form of an XML file. This approach is more complicated, as it includes reading data from a file that most often can not be fit into computers memory - size of XML containing articles from the Wikipedia often span across multiple gigabytes of data. Knowing that this file can not be read in a standard way, another approach was proposed. Python's XML.SAX module allows to treat an XML file as a stream of data and parse it using own handlers. For this purpose the WikiContentHandler class was designed and implemented. After reading each article it converts them into the Page class objects and puts them into the queue for further processing.

\begin{lstlisting}[language=Python, caption={Reader.\_read\_from\_file() - reading documents from XML file}, label={lst:reader:xml}]
def _read_from_file(self):
    wiki_handler = WikiContentHandler(self._q_unparsed_docs)
    sax_parser = xml.sax.make_parser()
    sax_parser.setContentHandler(wiki_handler)

    try:
        data_source = open('../data/wiki_dump.xml')
        sax_parser.parse(data_source)
        LOG.info('Parsed {0} items'.format(wiki_handler.items_saved))
    except PageLimitException as page_limit_exception:
        LOG.info(page_limit_exception)
    except KeyboardInterrupt:
        exit()
    finally:
        # A pill for other threads
        self._q_unparsed_docs.put(None)
\end{lstlisting} \label{impl-reader-xml}

Both cases are finished with putting None value to the queue. It is a signal for other processes listening on the queue that all documents were read from source and put to the queue.

In the next step the loaded documents are parsed. Since the Reader process is sending documents for processing one by one from the very begining of reading data source, a self-contained instances of the Parser class are started in seperate processes along with it. For better efficency multiple processes of the Parser class are created. \\

Parser process logic is executed inside \textit{run()} method presented in listing \ref{lst:parser:run}. It runs a loop until \textit{\_process\_page()} method (listing \ref{lst:parser:process}) returns None value, which is send by the Reader class object once all documents were read. If parsed document is received, it will be added to the list of documents parsed by this process.

 In other case the \textit{\_signal\_end\_processing()} method is called to notify all other processes that all documents in this process were parsed. Once list contains all documents that were assigned to this process, a \_tfidf() (listing \ref{lst:parser:tfidf}) method is called.

\begin{lstlisting}[language=Python, caption={Parser.run() - Parser class process main method}, label={lst:parser:run}]
def run(self):
    self.parsed_pages_num = 0
    parsed_pages = []
    while True:
        page = self._process_page()
        if page:
            parsed_pages.append(page)
        else:
            self._signal_end_processing()
            break

    self._tfidf(parsed_pages)
\end{lstlisting}

The Parser.\_process\_page() presented in listing \ref{lst:parser:process} is called for every single object send from the Reader process using queue. It is responsible for receiving and processing received documents. If document is received, its text is tokenized and lemmatized, and for each token a TF value is calculated in \textit{Page.create\_tokens()} method. All created tokens are send to the IDF process (IDF process is responsible for counting IDF value for each token, which then will be used to calculate TF-IDF values), where they are aggregated and counted. The Reader process sends None object after all documents were loaded from source, in which case the None value is returned to notify calling method that processing work was finished.

\begin{lstlisting}[language=Python, caption={Parser.\_process\_page() - Processing of unparsed document}, label={lst:parser:process}]
def _process_page(self):
    page = self._queue_unparsed_docs.get()
    if page is None:
        return None
    page.create_tokens()
    for token in page.tokens:
        self._pipe_tokens_to_idf_child.send(token.stem)
    page.content_clean()
    self.parsed_pages_num += 1
    return page
\end{lstlisting}

Once list in \textit{Parser.run()} (listing \ref{lst:parser:run}) contains all documents that were assigned to this process, a \_tfidf() (listing \ref{lst:parser:tfidf}) method is called. First, a lock in a form of multiprocessing event is checked. If event is set to low state its \textit{wait()} method causes calling process to stop and wait until event is set to high state. Once lock is raised, process starts receiving pairs of token and IDF values, which allows to calculate a TF-IDF value for every token in the document and create a feature matrix. Parsed document is sent to the main process as soon as all values are calculated for it.

\begin{lstlisting}[language=Python, caption={Parser.\_tfidf() - calculating TD-IDF values for tokens in documents}, label={lst:parser:tfidf}]
def _tfidf(self, parsed_pages):
    print('Process {0} waiting on IDF to finish...'.format(self.pid))
    self._event.wait()
    recv_tokens = self._pipe_tokens_to_processes_child.recv()
    print('Process {0} received {1} tokens from IDF'.format(
        self.pid, len(recv_tokens)))
    for page in parsed_pages:
        for token in page.tokens:
            try:
                token.idf = recv_tokens[token.stem]
                page.tfidf[token.stem] = token.calc_tf_idf()
            except KeyError as ke:
                print('error', token)
        self._queue_parsed_docs.put(page)
    # sending process-end pill
    self._queue_parsed_docs.put(None)
\end{lstlisting}

\subsubsection{Calculating distances}
Calculating distances, or the similarity between documents, is a crucial pre-requirement for clusterization and classification. It is conducted after all documents have been parsed and the feature matrixes are available. As a result, the matrix of distances is created, where row and column numbers corresponds to document IDs. For better efficiency multiple processes can be assigned to this task.

Task orchestration is conducted in \textit{Main.distance()} method presented in listing \ref{lst:main:distance}. It spawns desired number of Distance class processes that will calculate distances for assigned portion of work. All processes have access to shared memory object of multiprocessing.Array class. It is an array implementation that allows to share data with read and write permissions between processes. As long as every process performs write operations on seperate cells of array, there is no performance degredation.

\begin{lstlisting}[language=Python, caption={Main.distance() - Main process method for conducting distance calculations}, label={lst:main:distance}]
def distance(self):
    global distances
    distances = multiprocessing.Array('d', (largest_id+1)*(largest_id+1))

    dist_ps = []
    for i in range(PROCESSES):
        dist_p = Distance(
            iteration_offset=i,
            iteration_size=PROCESSES,
            distances=distances,
            largest_id=largest_id,
            parsed_docs=parsed_docs
        )
        dist_p.start()
        dist_ps.append(dist_p)

    for dist_p in dist_ps:
        dist_p.join()
\end{lstlisting}

Internal operations of single process during document similarity calculations are presented in listing \ref{lst:distance:run}. Every process starts with a row which ID is equal to process ID. Upon completition of a single row, counter is increased by the number of processes working on this task. Because distance is a commutative property it is calculated only once for a pair of documents X and Y and is inserted at once under X:Y and Y:X location in the matrix. 

\begin{lstlisting}[language=Python, caption={Distance.run() - Distance class process main method}, label={lst:distance:run}]
def run(self):
    row = self.iteration_offset
    while row < (self.largest_id + 1):
        try:
            doc1 = self.parsed_docs[row]
            self.distances[Utils.coord_2d_to_1d(
	            row, row, (self.largest_id + 1))] = 1.0
            for col in range(row):
                distance = 0.0
                try:
                    doc2 = self.parsed_docs[col]
                    distance = Utils.calc_distance(doc1, doc2)
                except:
                    distance = -2
                self.distances[Utils.coord_2d_to_1d(
	                col, row, (self.largest_id + 1))] = distance
                self.distances[Utils.coord_2d_to_1d(
	                row, col, (self.largest_id + 1))] = distance
        except:
            # there is no document with such ID, fill it with -1
            # distances
            for col in range(row+1):
                self.distances[Utils.coord_2d_to_1d(
	                col, row, (self.largest_id + 1))] = -1
                self.distances[Utils.coord_2d_to_1d(
	                row, col, (self.largest_id + 1))] = -1
        row += self.iteration_size
\end{lstlisting}

\subsection{Clusterization} \label{impl:clusterization}

The implementation of k-means clustering algorithm presented in listing \ref{lst:main:cluster} is a continuous cooperation between master and slave processes. Master process is responsible for initializing communication channels, for scheduling workloads to slave processes and also for aggregating work done by slaves, assigning documents to groups, rescheduling all work and sending new group centers to slave processes.

At first, initial centers must be generated. It is done in \textit{Utils.initialize\_cluster\_centers()} method, which returns a set of randomly selected documents that will act as initial cluster centers. The number of centers used during clusterization process can be predefined by changing \textit{clusterization/centers} value in the configuration file. Centers are instances of \textit{ClusterCenter} class, that holds different pieces of information like groups central document ID and the list of all documents that were assigned to this center.

All centers are sent to slave processes in order to find the closest center for given documents. Once slave processes send back results, master process aggregates all results and attempts to define more appropriate center in each cluster by looking for a document with distance value closest to the average of distances in whole cluster. 

Whole process is repeated until one of conditions is met: either there was no change at the end of iteration or the iterations number is over the limit. Default value of iterations limit is \textit{100}, but it can be changed in the configuration file.

\begin{lstlisting}[language=Python, caption={Main.cluster() - Main process method for conducting clustering}, label={lst:main:cluster}]
def cluster(self):
    global distances
    global parsed_docs

    center_num = int(CONF['clusterization']['centers'])
    centers = Utils.initialize_cluster_centers(
        center_num=center_num,
        start=0,
        end=largest_id,
        parsed_docs=parsed_docs
    )
    new_centers = {}

    cluster_ps = []
    pipe_receive_results, pipe_send_results = multiprocessing.Pipe()

    for pid in range(PROCESSES):
        pipe_send_centers, pipe_receive_centers = multiprocessing.Pipe()
        cluster_p = Clusterization(
            offset=pid,
            shift=PROCESSES,
            pipe_send_centers=pipe_send_centers,
            pipe_receive_centers=pipe_receive_centers,
            parsed_docs=parsed_docs,
            distances=distances,
            largest_id=largest_id,
            pipe_send_results=pipe_send_results,
        )
        cluster_p.start()
        cluster_ps.append(cluster_p)

    iteration = 0
    iteration_limit = int(CONF['clusterization']['iterations_limit'])
    changed = False
    docs_num = 0
    while iteration < iteration_limit:
        docs_num = 0
        for cluster_p in cluster_ps:
            cluster_p.pipe_send_centers.send(list(centers.keys()))
        new_centers = {}
        not_finished = PROCESSES
        while not_finished:
            recv = pipe_receive_results.recv()
            if not recv:
                not_finished -= 1
            else:
                cid = recv['cid']
                did = recv['did']
                dist = recv['dist']
                centers[cid].add_doc(doc_id=did, distance=dist)
                parsed_docs[did].center_id = cid
        for cid in centers:
            docs_num += len(centers[cid].doc_ids)
        for cid in centers:
            new_cid = centers[cid].find_closest_doc_to_average()
            if not centers[cid].center_changed:
                new_cid = cid
            new_center = ClusterCenter()
            new_center.doc_ids = {}
            new_center.pre_doc_ids = {}
            new_center.center_id = new_cid
            new_centers[new_cid] = new_center
            if cid != new_cid:
                changed = True

        if not changed:
            break
        centers = new_centers
        iteration += 1

    for cluster_p in cluster_ps:
        cluster_p.pipe_send_centers.send(None)
        cluster_p.join()
    print('Docs sum: ', docs_num)
    print('parsed docs: ', len(parsed_docs))
    print('centers:', len(centers))
\end{lstlisting}

Clusterization class process conducts search for closest documents. It awaits for master process to send a set of selected cluster centers. Each k-means iteration is calculated within a single iteration in each process in \textit{\_find\_closest\_docs\_to\_center()} method presented in listing \ref{lst:clusterization:closest}. Each process awaits for master process to send new centers (\textit{\_receive\_centers()} method) and then it starts computation presented in \ref{lst:clusterization:closest}.

\begin{lstlisting}[language=Python, caption={Clusterization.run() - Clusterization class process main method}, label={lst:clusterization:run}]
def run(self):
    while True:
        self._receive_centers()
        if not self.centers:
            break
        self._find_closest_docs_to_center()
\end{lstlisting}

\textit{\_find\_closest\_docs\_to\_center()} presented in listing \ref{st:clusterization:closest} iterates through all documents that were sent to process and responds with a dictionary that contains information about document ID, its closest center and the distance between these two. Closest center and distance is calculated in \textit{\_closest\_center\_id\_for\_doc\_id()} method in listing \ref{lst:clusterization:centerdoc}.

\begin{lstlisting}[language=Python, caption={Clusterization.\_find\_closest\_docs\_to\_center() - Finding closest center for a document during clusterization, part 1}, label={lst:clusterization:closest}]
def _find_closest_docs_to_center(self):
    did = self.offset
    while did < self.largest_id:
        try:
            ret = self._closest_center_id_for_doc_id(did)
            if ret:
                closest_cid, distance = ret
                self.pipe_send_results.send({
                    'cid': closest_cid,
                    'did': did,
                    'dist': distance,
                })
            did += self.shift
        except Exception as ex:
            print(ex)
    self.pipe_send_results.send(None)
\end{lstlisting}

Last method used in document clusterization is \textit{ \_closest\_center\_id\_for\_doc\_id()} method presented in listing \ref{lst:clusterization:centerdoc}. It takes a single document and compares distances between it and all cluster centers. Because distance is a function of similarity in this implementation, center with highest value is selected.

\begin{lstlisting}[language=Python, caption={Clusterization.\_closest\_center\_id\_for\_doc\_id() - Finding closest center for a document during clusterization, part 2}, label={lst:clusterization:centerdoc}]
def _closest_center_id_for_doc_id(self, did):
    try:
        test = self.parsed_docs[did]
    except:
        return None
    closest_cid = None
    closest_cid_distance = -100
    for cid in self.centers:
        cid_distance = self.distances[Utils.coord_2d_to_1d(cid, did,
                                                           self.largest_id)]
        if closest_cid_distance < cid_distance:
            closest_cid = cid
            closest_cid_distance = cid_distance
    if closest_cid is None:
        raise Exception('Error in finding closest '
                        'distance doc_id:{0}'.format(did))
    return closest_cid, closest_cid_distance
\end{lstlisting}

\subsection{Classification}

Classification is run in the final phase of application. It consist of two parts - kNN and SVM. First one is used for general decision that narrows the possible class memberships on which the second one will work on, giving more precise decision.

\subsubsection{kNN}
kNN's classification is run in parallel in seperate processes. Main process is responsible for initializing communication, launching processes and dividing work between them. It bases on informations provided by clusterization phase described in detail in section \ref{impl:clusterization}.

kNN goal is to find \textit{k} nearest number for selected doument. \textit{k} number is customizable and can be changed in configuration file under \textit{classification/k} key. Document can be pointed out in configuration file as well by assigning it's ID to \textit{classification/new\_doc\_start\_id} key. In current implementation documents for classification can be only read from database.

New document, before being classified, must be prepared in the same way as documents before distance calculations were parsed. It is done in \textit{\_prepare\_new\_doc()} method presented in listing \ref{lst:classification:newdoc}.

Next, each of worker processes (listing \ref{lst:classification:run}) receives a sub set of existing documents to calculate distances between them and the new document. Resulst are sent back to the main process, were thay are aggregated and the \textit{k} closest ones are selected. Label for new document is created basing on the most frequent membership in documents in the selected group.

\begin{lstlisting}[language=Python, caption={Main.classify() - Main process method for conducting kNN classification}, label={lst:main:classify}]
def classify(self):
    Db.init()
    session = Db.create_session()
    docs = session.query(Models.Doc).filter(
        Models.Doc.id == int(CONF['classification']['new_doc_start_id'])
    )
    if docs.count():
        for doc in docs:
            new_doc = self._prepare_new_doc(doc)
            class_distances = multiprocessing.Array('d', (largest_id + 1))
            class_ps = []
            for i in range(PROCESSES):
                class_p = Classification(
                    iteration_offset=i,
                    iteration_size=PROCESSES,
                    class_distances=class_distances,
                    largest_id=largest_id,
                    parsed_docs=parsed_docs,
                    new_doc=new_doc,
                )
                class_p.start()
                class_ps.append(class_p)

            for class_p in class_ps:
                class_p.join()

            id_dist = []
            for i in range(largest_id + 1):
                try:
                    item = {
                        'id': i,
                        'distance': class_distances[i],
                        'class': parsed_docs[i].center_id
                    }
                    id_dist.append(item)
                except KeyError:
                    pass

            # finding most frequent center in close neighborhood
            id_dist.sort(key=lambda x: x['distance'], reverse=True)
            k_id_dist = id_dist[:int(CONF['classification']['k'])]
            classes = [c['class'] for c in k_id_dist]
            counted_classes = Counter(classes)
            new_doc.center_id, _ = counted_classes.most_common(1)[0]
            LOG.info('New doc ({0}) classified as belonging to {1} : {2}'.
                     format(new_doc.title, new_doc.center_id,
                     parsed_docs[new_doc.center_id].title))
            print([parsed_docs[doc].title for doc in parsed_docs if
                   parsed_docs[doc].center_id ==
                   new_doc.center_id])

    else:
        LOG.info('No documents to classify')
\end{lstlisting}

To be able to find closest neighbors to new document, it must be prepared. It is done in \textit{\_prepare\_new\_doc()} presented in listing \ref{lst:classification:newdoc}. Document is transformed into Page class object and its feature matrix is created. For each token a TF-IDF must be calculated. TF value is calculated for a single document, so no further action was required. In the case of IDF it was necessary to look up token's IDF value in existing structures, as by rule this value is releated to word frequency in whole corpus.

\begin{lstlisting}[language=Python, caption={prepare\_new\_doc() - Method used in classification for parsing new document}, label={lst:classification:newdoc}]
def _prepare_new_doc(self, doc):
    page = Page()
    page.title = doc.title
    page.content = doc.text
    page.create_tokens()
    # import tokens IDF values from already classified documents
    for page_token in page.tokens:
        try:
            page_token.idf = tokens_idf[page_token.stem]
        except KeyError:
            # token did not appear in previous documents
            page_token.idf = 1 + math.log((len(parsed_docs) + 1) / 1.0,
                                          math.e)
        finally:
            page.calc_tokens_tfidf()
    return page
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Classification.run() - Classification class process main method}, label={lst:classification:run}]
def run(self):
    doc_id = self.iteration_offset
    while doc_id < (self.largest_id + 1):
        try:
            existing_doc = self.parsed_docs[doc_id]
            distance = Utils.calc_distance(self.new_doc, existing_doc)
            self.class_distances[doc_id] = distance
        except:
            # there is no document with such ID, distance is -1
            self.class_distances[doc_id] = -1

        doc_id += self.iteration_size
\end{lstlisting}

\subsubsection{SVM}
In this implementation the SVM algorithm works on a narrowed corpus that was selected during the kNN classification phase. According to presented methodology (\ref{methodology:svm}) the \textit{one-versus-one} approach was chosen to be incorporated in this system. The function (\ref{lst:main:svm}) is run in the main process of application and is responsible for data partitioning and scheduling tasks between worker processes. It generates all possible combinations of pairs of classes and assigns documents to chosen classes creating a task that is then put to the queue. Once worker process has finished working on a task, it sends back results to master process to aggregate. After receiving all results, main process choses the most frequent result as a label for the new document.

\begin{lstlisting}[language=Python, caption={Main.classify\_svm() - Main process method for conducting SVM classification}, label={lst:main:svm}]
def classify_svm(self):
    if len(self.k_classes) < 2:
        LOG.info('There is only one possible class, not need to run SVM')
        return

    LOG.debug('Document classes for SVM: {0}'.format(self.k_classes))

    ### Gather all documents, grouped into classes
    classes_doc = {}
    for class_id in self.k_classes:
        # select *ALL* documents that belong to classes indicated by kNN
        docs_id_in_class = [parsed_docs[doc_id].id for doc_id in
                         parsed_docs if
                         parsed_docs[doc_id].center_id == class_id]
        classes_doc[class_id] = docs_id_in_class

    pair_queue = multiprocessing.Queue()
    result_queue = multiprocessing.Queue()
    results = {}
    svm_ps = []
    for pid in range(PROCESSES):
        svm_p = SVM(
            pair_queue=pair_queue,
            result_queue=result_queue,
            classes_doc=classes_doc,
            parsed_docs=parsed_docs,
            new_doc=self.new_doc
        )
        svm_p.start()
        svm_ps.append(svm_p)

    # generate n(n-1)/2 class pairs
    combinations = itertools.combinations(self.k_classes, 2)

    for pair in combinations:
        LOG.debug('Sending class pair: {0}'.format(pair))
        pair_queue.put(pair)

    for i in range(PROCESSES):
        pair_queue.put(None)

    not_finished = PROCESSES
    while not_finished:
        res = result_queue.get()
        if not res:
            not_finished -= 1
            continue
        LOG.debug('Received SVM pair: {0} {1} with result {2}'.format(
            res['class1'], res['class2'], res['result']))
        try:
            results[res['result']] += 1
        except:
            results[res['result']] = 1

    class_id, _ = sorted(results.items(), key=operator.itemgetter(1))[-1]
    LOG.info('SVM group id: {0} ({1})'.format(
        class_id,
        parsed_docs[class_id].title),
    )

    for svm_p in svm_ps:
        svm_p.join()

    LOG.info('Finished ciassification')
\end{lstlisting}

Method presented on listing \ref{lst:svm:run} is the main entry point for every worker process. It invokes \textit{\_svm\_pair()} (\ref{lst:svm:recv}) function as long as \textit{\_recv()} (\ref{lst:svm:svm}) returns value different than None.

\begin{lstlisting}[language=Python, caption={SVM.run() - SVM class process main method}, label={lst:svm:run}]
def run(self):
    while self._recv():
        self._svm_pair()
\end{lstlisting}

\textit{\_recv()} method (\ref{lst:svm:recv}) conducts receiving tasks for given worker thread as long as there is any object in the task queue. In this case, the None value is returned.

\begin{lstlisting}[language=Python, caption={SVM.\_recv() - SVM class process main method}, label={lst:svm:recv}]
def _recv(self):
    val = self.pair_queue.get()
    if not val:
        self.result_queue.put(None)
        return None
    self.current_pair = val
    return val
\end{lstlisting}

Method presented in the listing \ref{lst:svm:svm} represents the main logic of parallelized SVM algorithm execution.  Worker processes are given a pair of classes. Basing on linear kernel, SVM will classify the new document to one of them. Classification begins with creating a feature matrix that includes features gathered from existing documents that belong to one of two classes and the new document. Once matrix is prepared, SVM classifier is trained on old documents, which eventually allows new document to be classified. Most matching class is selected and the decision is sent back to master process for further evaluation.

\begin{lstlisting}[language=Python, caption={SVM.\_svm\_pair() - SVM class process main method}, label={lst:svm:svm}]
def _svm_pair(self):
    class1, class2 = self.current_pair

    class1_docs = self.classes_doc[class1]
    class2_docs = self.classes_doc[class2]
    all_docs = []
    all_docs.extend(class1_docs)
    all_docs.extend(class2_docs)

    tokens_template = {}
    class_distribution = []
    # create template for existing documents
    for doc_id in all_docs:
        doc = self.parsed_docs[doc_id]
        class_distribution.append(doc.center_id)
        for token in doc.tokens:
            tokens_template[token.stem] = 1

    for token in self.new_doc.tokens:
        tokens_template[token.stem] = 1
    tokens_template = list(tokens_template.keys())

    feature_matrix = []
    for doc_id in all_docs:
        feature_vector = []
        doc = self.parsed_docs[doc_id]
        for token in tokens_template:
            try:
                feature_value = doc.tfidf[token]
            except:
                feature_value = 0.0
            feature_vector.append(feature_value)
        feature_matrix.append(feature_vector)

    new_feature_vector = []
    for token in tokens_template:
        try:
            feature_value = self.new_doc.tfidf[token]
        except:
            feature_value = 0.0
        new_feature_vector.append(feature_value)

    class_distribution = [class1]*len(class1_docs)+[class2]*len(class2_docs)
    clf = svm.SVC(kernel='linear', C=1.0)
    clf.fit(feature_matrix, class_distribution)
    result = clf.predict(new_feature_vector)

    # send results
    self.result_queue.put({
        'class1': class1,
        'class2': class2,
        'result': result[0]
    })
\end{lstlisting}

\section{Conclusion}
This chapter presented detailed implementation of the system. It describes flow of the application in sequence diagrams and presents possible use cases. Attached class diagram reflects relation between logical blocks of code used in the system. Source code for all algorithms was included and described.