\chapter{Implementation} \label{implementation}

\section{Introduction}
Presented solution uses master-slave architecture in order to provide scalable platform with means of parallel task execution. This chapter describes its code implementation for all of the algorithms and methods used in text document classification process.


\section{Implementation}

\subsection{Preparing documents}

\subsubsection{Main process}
\begin{lstlisting}[language=Python, caption=Main process]
def parse(self):
    global parsed_docs
    global largest_id
    global tokens_idf
    
    # initialize communication
    
    queue_unparsed_docs = multiprocessing.Queue()
    queue_parsed_docs = multiprocessing.Queue()
    pipe_tokens_to_idf_parent, pipe_tokens_to_idf_child = multiprocessing.Pipe()
    pipes_tokens_to_processes_parent = []
    pipes_tokens_to_processes_child = []

    for i in range(PROCESSES):
	    pipe_tokens_to_processes_parent, pipe_tokens_to_processes_child = multiprocessing.Pipe()
	    pipes_tokens_to_processes_parent.append(pipe_tokens_to_processes_parent)
	    pipes_tokens_to_processes_child.append(pipe_tokens_to_processes_child)
    
    # additional pipe to transfer IDF values from IDF process to master
    pipe_idf_master_parent, pipe_idf_master_child = multiprocessing.Pipe()
    pipes_tokens_to_processes_parent.append(pipe_idf_master_parent)
    
    event = multiprocessing.Event()
    event.clear()
    
    # set up processes
    
    ps_reader = Reader(q_unparsed_docs=queue_unparsed_docs)
    ps_parsers = create_parsers(
	    queue_unparsed_documents=queue_unparsed_docs,
	    pipe_tokens_to_idf_child=pipe_tokens_to_idf_child,
	    event=event,
	    pipes_tokens_to_processes_child=pipes_tokens_to_processes_child,
	    queue_parsed_docs=queue_parsed_docs,
	    process_num=PROCESSES
    )
    ps_idf = IDF(
	    pipe_tokens_to_idf_parent=pipe_tokens_to_idf_parent,
	    docs_num=int(CONF['general']['item_limit']),
	    event=event,
	    pipes_tokens_to_processes_parent=pipes_tokens_to_processes_parent,
	    process_num=PROCESSES
    )
    
    # read all the articles from XML and do TF-IDF
    ps_reader.start()
    
    LOG.info("Started processing documents using {0} processes".format(PROCESSES))
    for ps_parser in ps_parsers:
	    ps_parser.start()
    ps_idf.start()
    
    # receive tokens IDF values from IDF process
    tokens_idf = pipe_idf_master_child.recv()
    
    ps_reader.join()
    ps_idf.join()
    
    # processes will not end until all the data is not received
    parsed_docs = self._receive_parsed_docs(queue_parsed_docs)
    
    for ps_parser in ps_parsers:
	    ps_parser.join()
\end{lstlisting}

\subsubsection{Reading documents from source}
\begin{lstlisting}[language=Python, caption=Reader class]


class Reader(multiprocessing.Process):
	def __init__(self, q_unparsed_docs):
		self._q_unparsed_docs = q_unparsed_docs
		super(self.__class__, self).__init__()

	def _read_from_file(self):
		wiki_handler = WikiContentHandler(self._q_unparsed_docs)
		sax_parser = xml.sax.make_parser()
		sax_parser.setContentHandler(wiki_handler)

		try:
			data_source = open('../data/wiki_dump.xml')
			sax_parser.parse(data_source)
			LOG.info('Parsed {0} items'.format(wiki_handler.items_saved))
	except PageLimitException as page_limit_exception:
		LOG.info(page_limit_exception)
	except KeyboardInterrupt:
		exit()
	finally:
		# A pill for other threads
		self._q_unparsed_docs.put(None)

	def _read_from_db(self):
		Db.init()
		session = Db.create_session()
		pages = session.query(Models.Doc).all()

		for page in pages:
			p = Page()
			p.id = page.id
			p.title = page.title
			p.content = page.text
			self._q_unparsed_docs.put(p)
		self._q_unparsed_docs.put(None)	

	def run(self):
		if str2bool(CONF['general']['load_from_db']):
			LOG.info('Loading from DB')
			self._read_from_db()
		else:
			self._read_from_file()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=XML parser]
class WikiContentHandler(xml.sax.ContentHandler):
    def __init__(self, q_unparsed_documents):
        self._path = []
        self._text = None
        self._title = None
        self._redirect = None
        self._items_limit = int(CONF['general']['item_limit'])
        self._classification_items_limit = int(
            CONF['classification']['item_limit'])
        self.items_saved = 0
        self._documents_saved = 0
        self._redirects_saved = 0
        self._q_unparsed_documents = q_unparsed_documents

        if str2bool(CONF['general']['save_to_db']):
            Db.init()
            Db.clean()  # database has to be cleaned every time to ensure
            # that IDs are unique
            self.session = Db.create_session()

    def startElement(self, name, attributes):
        if name == "page":
            assert self._path == []
            self._text = None
            self._title = None
            self._redirect = None
        elif name == "title":
            assert self._path == ["page"]
            assert self._title is None
            self._title = ""
        elif name == "redirect":
            assert self._path == ["page"]
            assert self._redirect is None
            self._redirect = attributes['title']
        elif name == "text":
            assert self._path == ["page"]
            assert self._text is None
            self._text = ""
        else:
            assert len(self._path) == 0 or self._path[-1] == "page"
            return

        self._path.append(name)

    def endElement(self, name):
        if len(self._path) > 0 and name == self._path[-1]:
            del self._path[-1]
        if name == "text":
            # We have the complete article: write it to db
            if not self._redirect:
                # Page
                page = Page()
                # reading is run in a single thread, so we can generate an
                # unique ID here
                page.id = self.items_saved
                page.content = self._text
                page.title = self._title
                self._q_unparsed_documents.put(page)

                if str2bool(CONF['general']['save_to_db']):
                    page = Models.Doc()
                    page.id = page.id = self.items_saved
                    page.text = self._text
                    page.title = self._title
                    self.session.add(page)
            else:
                pass
                # TODO do redirects carry any relevant information?
            self._monitor_progress()

    def _monitor_progress(self):
        self.items_saved += 1
        if not self._redirect:
            self._documents_saved += 1
        else:
            self._redirects_saved += 1

        if str2bool(CONF['general']['save_to_db']):
            if self.items_saved % 10:
                self.session.commit()

        if self.items_saved % (int(math.ceil(self._items_limit/10)) if int(
                math.ceil(self._items_limit/10)) > 0 else 1) == 0:
            LOG.debug('[{0:6.2f} %] Parsed {1} / {2} items'.format(
                self.items_saved / float(self._items_limit) * 100,
                self.items_saved, self._items_limit),
            )
        if self._items_limit and self.items_saved >= self._items_limit:
            if str2bool(CONF['general']['save_to_db']):
                self.session.commit()
            raise PageLimitException('Parser hit items limit ({0}), '
                                     'Parsed pages: {1}, '
                                     'Parsed redirects {2}'.format(
                                         self._items_limit,
                                         self._documents_saved,
                                         self._redirects_saved
                                         )
                                     )

    def characters(self, content):
        assert content is not None and len(content) > 0
        if len(self._path) == 0:
            return

        if self._path[-1] == "title":
            self._title += content
        elif self._path[-1] == "text":
            assert self._title is not None
            self._text += content
\end{lstlisting}

\subsubsection{Calculating tokens TF-IDF values }
\begin{lstlisting}[language=Python, caption=IDF process]
import math
import multiprocessing

from data.db import Db, Models
from utils.config import get_conf
from utils.general import str2bool

CONF = get_conf()


class IDF(multiprocessing.Process):
    def __init__(self, pipe_tokens_to_idf_parent, docs_num, event,
                 pipes_tokens_to_processes_parent, process_num):
        self._pipe_tokens_to_idf_parent = pipe_tokens_to_idf_parent
        self._docs_num = docs_num  # total number of documents
        self._event = event
        self._tokens = {}
        self._pipes_tokens_to_processes_parent = \
            pipes_tokens_to_processes_parent
        self.process_num = process_num
        super(self.__class__, self).__init__()

    def _receive_tokens(self):
        pills = 0
        while pills < self.process_num:
            msg = self._pipe_tokens_to_idf_parent.recv()
            if msg is None:
                pills += 1
                continue
            if msg in self._tokens.keys():
                self._tokens[msg] += 1
            else:
                self._tokens[msg] = 1

    def _calc_token_idf(self, token):
        # IDF(token) = 1 + log_e(Total Number Of Documents / Number Of
        # Documents with token in it)
        token_idf = 1 + math.log(self._docs_num / self._tokens[token],
                                 math.e)
        self._tokens[token] = token_idf
        return token_idf

    def run(self):
        session = None
        token_counter = None
        if str2bool(CONF['general']['save_to_db']):
            Db.init()
            session = Db.create_session()
            token_counter = 0

        self._receive_tokens()

        for token in self._tokens:
            token_idf = self._calc_token_idf(token)

            if str2bool(CONF['general']['save_to_db']):
                token_counter += 1
                t = Models.Token()
                t.stem = token
                t.idf = token_idf
                session.add(t)

                if token_counter % 50 == 0:
                    session.commit()
        if str2bool(CONF['general']['save_to_db']):
            session.commit()
        self._event.set()
        for pipe in self._pipes_tokens_to_processes_parent:
            pipe.send(self._tokens)
        print('IDF sent {0} tokens'.format(len(self._tokens)))

\end{lstlisting}

\subsection{Clusterization}
TODO

\subsection{Classification - kNN}
TODO

\subsection{Classification - SVM}
TODO
