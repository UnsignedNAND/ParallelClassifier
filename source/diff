diff --git a/source/core/main.py b/source/core/main.py
index 3c1a97d..0bb0a77 100644
--- a/source/core/main.py
+++ b/source/core/main.py
@@ -1,6 +1,7 @@
 import math
 import multiprocessing
 
+from collections import Counter
 from core.process.classification import Classification
 from core.process.clusterization import Clusterization
 from core.process.distance import Distance
@@ -20,6 +21,7 @@ parsed_docs = {}
 largest_id = -1
 process_num = int(CONF['general']['processes'])
 distances = None
+class_distances = None
 tokens_idf = {}
 
 
@@ -250,6 +252,8 @@ def _prepare_new_doc(doc):
                                           math.e)
             LOG.debug('Classification: token \'{0}\' is new.'.format(
                 page_token.stem))
+        finally:
+            page.calc_tokens_tfidf()
     return page
 
 
@@ -257,13 +261,54 @@ def _prepare_new_doc(doc):
 def classify():
     Db.init()
     session = Db.create_session()
-    docs = session.query(Models.Doc).filter(Models.Doc.id == 1)
+    docs = session.query(Models.Doc).filter(
+        Models.Doc.id == int(CONF['classification']['new_doc_start_id'])
+    )
     if docs.count():
         for doc in docs:
             LOG.info('Classifying "{0}"'.format(doc.title))
-            page = _prepare_new_doc(doc)
+            new_doc = _prepare_new_doc(doc)
+            class_distances = multiprocessing.Array('d', (largest_id + 1))
+            class_ps = []
+            for i in range(process_num):
+                class_p = Classification(
+                    iteration_offset=i,
+                    iteration_size=process_num,
+                    class_distances=class_distances,
+                    largest_id=largest_id,
+                    parsed_docs=parsed_docs,
+                    new_doc=new_doc,
+                )
+                class_p.start()
+                class_ps.append(class_p)
+
+            for class_p in class_ps:
+                class_p.join()
+
+            id_dist = []
+            for i in range(largest_id + 1):
+                try:
+                    item = {
+                        'id': i,
+                        'distance': class_distances[i],
+                        'class': parsed_docs[i].center_id
+                    }
+                    id_dist.append(item)
+                except KeyError:
+                    pass
+
+            # finding most frequent center in close neighborhood
+            id_dist.sort(key=lambda x: x['distance'], reverse=True)
+            k_id_dist = id_dist[:int(CONF['classification']['k'])]
+            classes = [c['class'] for c in k_id_dist]
+            counted_classes = Counter(classes)
+            new_doc.center_id, _ = counted_classes.most_common(1)[0]
+            LOG.info('New doc ({0}) classified as belonging to {1} : {2}'.
+                     format(new_doc.title, new_doc.center_id,
+                     parsed_docs[new_doc.center_id].title))
+
     else:
-        LOG.error('No documents to classify')
+        LOG.info('No documents to classify')
 
 
 if __name__ == '__main__':
diff --git a/source/core/process/classification.py b/source/core/process/classification.py
index 54f0962..e2e4b92 100644
--- a/source/core/process/classification.py
+++ b/source/core/process/classification.py
@@ -1,6 +1,28 @@
 import multiprocessing
 
+from core.utils import calc_distance
+
 
 class Classification(multiprocessing.Process):
-    def __init__(self):
+    def __init__(self, iteration_offset, iteration_size, class_distances,
+                 largest_id, parsed_docs, new_doc):
+        self.iteration_offset = iteration_offset
+        self.iteration_size = iteration_size
+        self.class_distances = class_distances
+        self.largest_id = largest_id
+        self.parsed_docs = parsed_docs
+        self.new_doc = new_doc
         super(self.__class__, self).__init__()
+
+    def run(self):
+        doc_id = self.iteration_offset
+        while doc_id < (self.largest_id + 1):
+            try:
+                existing_doc = self.parsed_docs[doc_id]
+                distance = calc_distance(self.new_doc, existing_doc)
+                self.class_distances[doc_id] = distance
+            except IndexError:
+                # there is no document with such ID, distance is -1
+                self.class_distances[doc_id] = -1
+
+            doc_id += self.iteration_size
diff --git a/source/core/process/distance.py b/source/core/process/distance.py
index 54ca498..2f1556a 100644
--- a/source/core/process/distance.py
+++ b/source/core/process/distance.py
@@ -34,7 +34,7 @@ class Distance(multiprocessing.Process):
                     try:
                         doc2 = self.parsed_docs[col]
                         distance = calc_distance(doc1, doc2)
-                    except:
+                    except IndexError:
                         distance = -2
                     self.distances[
                         coord_2d_to_1d(col, row, (self.largest_id + 1))
@@ -42,7 +42,7 @@ class Distance(multiprocessing.Process):
                     self.distances[
                         coord_2d_to_1d(row, col, (self.largest_id + 1))
                     ] = distance
-            except:
+            except IndexError:
                 # there is no document with such ID, fill it with -1
                 # distances
                 for col in range(row):
diff --git a/source/core/utils.py b/source/core/utils.py
index 997d1ef..ec7ba6a 100644
--- a/source/core/utils.py
+++ b/source/core/utils.py
@@ -45,7 +45,13 @@ def calc_distance(doc1, doc2):
         tfidf_2 = doc2.tfidf[token_2]
         d2 += math.pow(tfidf_2, 2)
     d2 = math.sqrt(d2)
-    return int(dot_product / (d1 * d2) * 1000) / 1000.0
+
+    try:
+        cos_similarity = int(dot_product / (d1 * d2) * 1000) / 1000.0
+    except ZeroDivisionError:
+        cos_similarity = 0.0
+
+    return cos_similarity
 
 
 def str_1d_as_2d(arr, size):
diff --git a/source/models/page.py b/source/models/page.py
index 9884a26..fcf1b33 100644
--- a/source/models/page.py
+++ b/source/models/page.py
@@ -67,3 +67,8 @@ class Page(object):
             reverse=True,
             key=lambda token: token.tf_idf
         ))[:num]
+
+    def calc_tokens_tfidf(self):
+        for token in self.tokens:
+            tfidf = token.calc_tf_idf()
+            self.tfidf[token.stem] = tfidf
diff --git a/source/wiki.conf b/source/wiki.conf
index 7055835..a9e465c 100644
--- a/source/wiki.conf
+++ b/source/wiki.conf
@@ -7,11 +7,13 @@ save_to_db=false
 load_from_db=false
 
 [clusterization]
-centers = 5
+centers = 3
 iterations_limit = 100
 
 [classification]
-item_limit = 5
+new_doc_start_id = 0
+item_limit = 1
+k = 1
 
 [db]
 #connection = mysql://root@localhost/wiki?charset=utf8mb4
