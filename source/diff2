diff --git a/source/core/main.py b/source/core/main.py
index 0bb0a77..44a5160 100644
--- a/source/core/main.py
+++ b/source/core/main.py
@@ -151,6 +151,7 @@ def cluster():
         docs_num=len(parsed_docs),
         distances=distances
     )
+    print('DUPA generated len(centers) : {0}'.format(len(centers)))
     pipe_results_parent, pipe_results_child = multiprocessing.Pipe()
     cluster_ps = []
     pipes_centers = []
@@ -185,6 +186,7 @@ def cluster():
         not_finished = process_num
         new_centers = {}
         for (pipe_center_parent,_) in pipes_centers:
+            print('DUPA sending len(centers) : {0}'.format(len(centers)))
             pipe_center_parent.send(centers)
 
         while not_finished:
@@ -246,7 +248,7 @@ def _prepare_new_doc(doc):
         try:
             # TODO increment total number of docs by 1
             page_token.idf = tokens_idf[page_token.stem]
-        except KeyError:
+        except:
             # token did not appear in previous documents
             page_token.idf = 1 + math.log((len(parsed_docs) + 1) / 1.0,
                                           math.e)
diff --git a/source/core/process/clusterization.py b/source/core/process/clusterization.py
index a76cbed..ed47109 100644
--- a/source/core/process/clusterization.py
+++ b/source/core/process/clusterization.py
@@ -26,9 +26,15 @@ class Clusterization(multiprocessing.Process):
                 closest_center = None
                 closest_center_distance = None
                 for center_id in self.centers:
-                    center_distance = self.distances[
-                        coord_2d_to_1d(center_id, doc_id, self.largest_id + 1)
-                    ]
+                    try:
+                        center_distance = self.distances[
+                            coord_2d_to_1d(center_id, doc_id, self.largest_id + 1)
+                        ]
+                    except Exception as ex:
+                        print('DUPA center_id : '.format(center_id))
+                        print('DUPA len(self.centers) : {0}'.format(len(
+                            self.centers)))
+                        raise ex
                     if closest_center_distance is None or \
                                     closest_center_distance < center_distance:
                         closest_center_distance = center_distance
diff --git a/source/core/process/distance.py b/source/core/process/distance.py
index 2f1556a..54ca498 100644
--- a/source/core/process/distance.py
+++ b/source/core/process/distance.py
@@ -34,7 +34,7 @@ class Distance(multiprocessing.Process):
                     try:
                         doc2 = self.parsed_docs[col]
                         distance = calc_distance(doc1, doc2)
-                    except IndexError:
+                    except:
                         distance = -2
                     self.distances[
                         coord_2d_to_1d(col, row, (self.largest_id + 1))
@@ -42,7 +42,7 @@ class Distance(multiprocessing.Process):
                     self.distances[
                         coord_2d_to_1d(row, col, (self.largest_id + 1))
                     ] = distance
-            except IndexError:
+            except:
                 # there is no document with such ID, fill it with -1
                 # distances
                 for col in range(row):
diff --git a/source/run.sh b/source/run.sh
index 1ff7e1b..e5754eb 100755
--- a/source/run.sh
+++ b/source/run.sh
@@ -5,5 +5,5 @@ python pwc.py \
     --debug \
     --parse \
     --distance \
-    --cluster \
-    --classify
+    --cluster
+#    --classify
diff --git a/source/wiki.conf b/source/wiki.conf
index a9e465c..236a950 100644
--- a/source/wiki.conf
+++ b/source/wiki.conf
@@ -1,13 +1,13 @@
 [general]
 path = /opt/parallel-wiki-classifier/
 processes = 4
-item_limit = 10
+item_limit = 1000
 #item_limit = 214736
 save_to_db=false
 load_from_db=false
 
 [clusterization]
-centers = 3
+centers = 600
 iterations_limit = 100
 
 [classification]
